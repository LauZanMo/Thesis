# 1 绪论

## 1.1 研究背景

​		四旋翼飞行器，是一种可以垂直起降，自由悬停的飞行器，因其拥有四个呈十字形交叉的螺旋桨的紧凑结构而得名，其相对于固定翼飞机有着起飞场地更加自由且能够悬停的优势，因此，四旋翼飞行器能够在军用或民用领域完成复杂的室内场景任务。

​		室外飞行器定位主要依赖GPS，GPS的搜星数量是飞行器定位的重要指标，但是，针对无GPS的室内环境下，飞行器则需要使用其他传感器进行定位与导航，同时，室内飞行器的体积与功耗也对算法的复杂程度有所限制，因此，如何在有限的体积与功耗的情况下，使用合适的传感器，提升飞行器的室内定位精度与鲁棒性，是飞行器室内定位的关键。

## 1.2 研究现状

​		国内外关于飞行器室内定位导航方案较多，以下列出几种应用于室内的解决方案：

​		超宽带定位导航。又称为UWB，其使用纳秒级非正弦波窄脉冲传输数据，有很高的时间分辨率，因此被运用到室内定位中，目前大部分室内定位产品运用的是Decawave推出的DW1000芯片，有集成度高，价格低廉的优点，但是，UWB也有信号容易被阻隔，需要提前布置的缺点，在地质勘探等高危场景下并不适用。

​		视觉定位导航。目前主流视觉SLAM前端方法有特征点法，光流法，直接法，后端方法有滤波器法和图优化法，相较于其他两种方法，特征点法有精度高的优点，但是需要对整张图像进行特征点提取，对算力要求更大，其他两种方法精度稍低，但是胜在速度更快，更适合算力较低的计算平台。图优化法相较于滤波器法精度更高，但是对于地图较大的场景维护速度会下降，滤波器法因为只需要考虑最近的数据，所以速度更快，但是会存在累积误差。目前优秀的视觉SLAM算法有ORB-SLAM，VINS，DSO等。

​		雷达定位导航。目前主流雷达定位方法主要是激光雷达和毫米波雷达，雷达测量精度高，在较暗环境下相较于视觉表现更好，在自动驾驶领域采用雷达方案的较多。但是，用于三维定位的雷达价格极高，且扫描频率低，不适合高速运动情况下的飞行器进行定位与导航。

​		本文旨在运用机载相机，对机载相机参数、硬件及定位导航策略进行优化，提升飞行器在无GPS情况下室内定位的精度与鲁棒性，从而运用到地质勘探，廊道巡逻等无GPS的复杂环境下。

## 1.3 主要内容与章节安排

​		本文主要的研究内容是设计一套携带鱼眼相机的飞行器SLAM系统，其中包括飞行器结构设计，鱼眼相机T265标定，针对T265在基于特征点的视觉SLAM进行参数优化和飞行器SLAM系统的整合。飞行器结构设计包括飞行器载荷计算与动力选用、飞行器机架构思与搭建；鱼眼相机T265标定包括鱼眼相机投影模型与畸变模型的选用、单目鱼眼相机的标定、双目鱼眼相机的标定；针对T265在基于特征点的视觉SLAM进行参数优化包括调整ORB提取特征点的数量、尺度因子、图像金字塔的数量以及提取Fast角点的阈值；飞行器SLAM系统的整合包括飞行器参数的调优、mavlink通信协议的构建、ROS网络的构建。

​		本文第一章介绍了室内飞行器定位的研究背景和研究现状，第二章分析了鱼眼相机误差模型和标定方法，第三章讲述了基于特征点的ORB-SLAM3视觉SLAM算法，第四章介绍了携带鱼眼相机的飞行器SLAM系统的设计与实现，第五章讲述了对设计的飞行器SLAM系统的性能进行分析，第六章则是对设计的飞行器SLAM系统进行总结，并提出对该系统的改进思路以及对未来的展望。

# 2 双目鱼眼相机误差分析及标定方法

​		鱼眼镜头，是广角镜头中的一种，其焦距小于16mm而且视场角接近或等于180°。在鱼眼镜头拥有着大视场角的优势的同时，其也有着因为光学原理所产生形变的缺点。因此，在近年火热的即时建图与定位(SLAM)研究中，运用最多的是基于针孔相机原理的标准镜头，鱼眼镜头运用并不广泛。

​		基于四旋翼飞行器的视觉SLAM难点有：飞行器相较于小车、手持等场景抖动更大，实时性要求更高，因此，鲁棒的相机位姿估计以及高实时性，是基于飞行器的视觉SLAM必须具备的特点。使用多个鱼眼相机进行刚性耦合，不仅能增加视场角，获取更加丰富的环境特征信息，还能通过相机系统的转换矩阵这一冗余信息恢复地图的尺度，增加位姿估计的鲁棒性以及特征点的持续追踪概率，进而达到飞行器视觉高精度SLAM的目标。

​		由于透视投影模型不适合鱼眼镜头，所以我们采用了一种更加灵活的径向对称的投影模型。

## 2.1 鱼眼相机模型

​		针孔相机的透视投影模型可由以下公式描述：
$$
r=f\tan\theta \tag{2.1 i}
$$
​		其中$\theta$是主光轴与入射光线的夹角，$r$是像面与主点的距离，$f$是镜头的焦距。鱼眼镜头则是经常被设计成符合以下的透视模型之一：
$$
r=2f\tan(\theta/2)\tag{2.2 ii}
$$

$$
r=f\theta\tag{2.3 iii}
$$

$$
r=2f\sin(\theta/2)\tag{2.4 iv}
$$

$$
r=f\sin(\theta)\tag{2.5 v}
$$

​		上述四个模型中最常见的模型为等距投影$(2.3)$，不同的投影模型的曲线如图2.1所示，针孔相机模型与鱼眼相机模型在几何上的差异如图2.2所示。

![image-20210411155256476](C:\Users\LiuZW\AppData\Roaming\Typora\typora-user-images\image-20210411155256476.png)

​		然而，真实的镜头并不完全遵从该建模，为了对所有的鱼眼相机都具有普适性，投影的一般式被设计如下：
$$
r(\theta)=k_1\theta+k_2\theta^3+k_3\theta^5+k_4\theta^7+k_5\theta^9+\cdots\tag{2.6}
$$
​		通常只需要保留前五个系数即可在有限的计算下，式$(2.6)$可以对任何不同的投影曲线有着良好的估计，若$f$为入射光线与归一化的图像坐标之间的映射关系，则有：
$$
\left(\begin{matrix}
x\\
y
\end{matrix}\right)
=r(\theta)
\left(\begin{matrix}
\cos\varphi\\
\sin\varphi
\end{matrix}\right)
=f(\phi)\tag{2.8}
$$

​		考虑到真实镜头并不会理想地对称，加入轴向与径向畸变：
$$
\Delta_r(\theta,\varphi)=
(l_1\theta+l_2\theta^3+l_3\theta^5)
(i_1\cos\varphi+i_2\sin\varphi+i_3\cos2\varphi+i_4sin2\varphi)\tag{2.9}
$$

$$
\Delta_{\mathrm{t}}(\theta, \varphi)=\left(m_{1} \theta+m_{2} \theta^{3}+m_{3} \theta^{5}\right)\left(j_{1} \cos \varphi+j_{2} \sin \varphi+j_{3} \cos 2 \varphi+j_{4} \sin 2 \varphi\right)\tag{2.10}
$$

​		代入$(2.8)$可得
$$
\mathbf{x}_{\mathrm{d}}=r(\theta) \mathbf{u}_{r}(\varphi)+\Delta_{r}(\theta, \varphi) \mathbf{u}_{r}(\varphi)+\Delta_{t}(\theta, \varphi) \mathbf{u}_{\varphi}(\varphi)\tag{2.11}
$$
​		其中$\mathbf{u}_{r}(\varphi)$和$\mathbf{u}_{\varphi}$分别是轴向与径向的单位向量。将相机平面的坐标转换到像平面，则有：
$$
\left(\begin{array}{l}
u \\
v
\end{array}\right)=\left[\begin{array}{cc}
m_{u} & 0 \\
0 & m_{v}
\end{array}\right]\left(\begin{array}{l}
x_{\mathrm{d}} \\
y_{\mathrm{d}}
\end{array}\right)+\left(\begin{array}{l}
u_{0} \\
v_{0}
\end{array}\right)=\mathcal{A}\left(\mathrm{x}_{\mathrm{d}}\right)\tag{2.12}
$$
​		其中$(u_0,v_0)^T$是主点，$m_u$和$m_v$单位是像平面水平方向和垂直方向的单位像素。由$(2.11)$$(2.12)$联立可得总的鱼眼相机模型：
$$
\mathbf{m}=\mathcal{P}_{\mathrm{c}}(\Phi)\tag{2.13}
$$

​		其中$\mathbf{m}=(u,v)^T$，因此，鱼眼相机总共有$23$个参数需要估计，以$\mathbf{p_{23}}$表示。

## 2.2 单目鱼眼相机标定

​		单目鱼眼相机标定过程总共包含四步。假设相机从$N$个视角观测到$M$个特征点，对于每个视角，若有旋转矩阵$R_j$和位移向量$t_j$描述从相机相对于标定板的位置，则有：
$$
\mathbf{X}_{\mathbf{c}}=\mathbf{R}_{j} \mathbf{X}+\mathbf{t}_{j}, \quad j=1, \ldots, N\tag{2.14}
$$
​		设标定板与$XY$平面重合，则特征点的坐标可表示为$\mathbf{X^i}=(X^i,Y^i,0)$，相应的齐次坐标可表示为$\mathbf{x_p^j}=(X^i,Y^i,1)^T$，在第$j$个视角中的观测坐标为$\mathbf{m_j^i}=(u_j^i,v_j^i)^T$。标定过程的前三步只与六个相机内参有关，缩写为$\mathbf{p}_{6} \hat{=}\left(k_{1}, k_{2}, m_{u}, m_{v}, u_{0}, v_{0}\right)$，其他的参数仅与最后一步有关。

### 2.2.1 内参初始化

​		鱼眼模型$r=k_1\theta+k_2\theta^3$中$k_1$和$k_2$的初始值，是由生产厂商提供的公称焦距$f$和视场角$\theta_{max}$计算所得，并可通过$r_{max}=k_1\theta_{max}+k_2\theta_{max}^3$获取相机平面的图像半径。

​		对于鱼眼镜头，真实图像只在图像帧的一个类圆区域之中，在像平面中，真实图像则在一个椭圆区域之中，有：
$$
\left(\frac{u-u_{0}}{a}\right)^{2}+\left(\frac{v-v_{0}}{b}\right)^{2}=1\tag{2.15}
$$
​		同时还有$m_u=a/r_{max}$和$m_v=b/r_{max}$，因此，可以通过上述计算$\mathbf{p_6}$中的$m_u$，$m_v$，$u_0$和$v_0$。

### 2.2.2 计算逆投影和单应矩阵

​		结合内参$\mathbf{p_6}$，将观测点$\mathbf{m_j^i}$逆投影到以相机为原点的单位球面上，这些点以$\tilde{\mathbf{x}}_{j}^{i}$表示，因为观测点从标定板到单位球面的变换属于透视投影，所以存在单应矩阵$\mathbf{H_j}$有$s\tilde{\mathbf{x}}_{j}^{i}=\mathbf{H_j}\mathbf{x_p^i}$。

​		以下是对于任意视角$j$的单应矩阵$\mathbf{H_j}$的计算方法：

1. 通过计算归一化图像坐标来逆投影特征点：

$$
\left(\begin{array}{l}
x_{j}^{i} \\
y_{j}^{i}
\end{array}\right)=\left[\begin{array}{cc}
1 / m_{u} & 0 \\
0 & 1 / m_{v}
\end{array}\right]\left(\begin{array}{c}
u_{j}^{i}-u_{0} \\
v_{j}^{i}-v_{0}
\end{array}\right)\tag{2.16}
$$

然后将之转化为极坐标$\left(r_{j}^{i}, \varphi_{j}^{i}\right)=\left(x_{j}^{i}, y_{j}^{i}\right)$，最后，通过求解一元三次方程$k_{2}\left(\theta_{j}^{i}\right)^{3}+k_{1} \theta_{j}^{i}-r_{j}^{i}=0$得到值$\theta_{j}^{i}$。

2. 令$\tilde{\mathbf{x}}_{j}^{i}=\left(\sin \varphi_{j}^{i} \sin \theta_{j}^{i}, \cos \varphi_{j}^{i} \sin \theta_{j}^{i}, \cos \theta_{j}^{i}\right)$。
3. 通过线性归一化算法，由$\tilde{\mathbf{x}}_{j}^{i} \leftrightarrow \mathbf{x}_{\mathrm{p}}^{i}$之间的映射关系计算单应矩阵$\mathbf{H_j}$的初始估计值。
4. 通过最小二乘减小$\sum_{i} \sin ^{2} \alpha_{j}^{i}$来优化单应矩阵$\mathbf{H_j}$，其中$\alpha_{j}^{i}$是单位向量$\tilde{\mathbf{x}}_{j}^{i}$与$\hat{\mathbf{x}}_{j}^{i}$的夹角。

### 2.2.3 外参初始化

​		相机外参的初始值是从单应矩阵分解而来的，有：
$$
s \tilde{\mathbf{x}}_{j}^{i}=\left[\begin{array}{ll}
\mathbf{R}_{j} & \mathbf{t}_{j}
\end{array}\right]\left(\begin{array}{c}
X^{i} \\
Y^{i} \\
0 \\
1
\end{array}\right)=\left[\begin{array}{lll}
\mathbf{r}_{j}^{1} & \mathbf{r}_{j}^{2} & \mathbf{t}_{j}
\end{array}\right]\left(\begin{array}{c}
X^{i} \\
Y^{i} \\
1
\end{array}\right)\tag{2.17}
$$
​		其中$\mathbf{H}_{j}=\left[\mathbf{r}_{j}^{1} \mathbf{r}_{j}^{2} \mathbf{t}_{j}\right]$，另有：
$$
\mathbf{r}_{j}^{1}=\lambda_{j} \mathbf{h}_{j}^{1}, \quad \mathbf{r}_{j}^{2}=\lambda_{j} \mathbf{h}_{j}^{2}, \quad \mathbf{r}_{j}^{3}=\mathbf{r}_{j}^{1} \times \mathbf{r}_{j}^{2}, \quad \mathbf{t}_{j}=\lambda_{j} \mathbf{h}_{j}^{3}\tag{2.18}
$$
​		其中$\lambda_{j}=\operatorname{sign}\left(H_{j}^{3,3}\right) /\left\|\mathbf{h}_{j}^{1}\right\|$，由于存在估计误差，计算得到的旋转矩阵不是正交矩阵，所以需要使用奇异值分解的方法计算最接近的正交矩阵，并令其作为$\mathbf{R}_{j}$的初始估计值。

### 2.2.4 投影误差最小化

​		鱼眼模型参数$\mathbf{p_{23}}$的除$\mathbf{p_6}$之外的其他参数初始值设置为$0$，使用式$(2.11)$、$(2.12)$和$(2.14)$来计算特征点满足$\hat{\mathbf{m}}_j^i=\mathcal{P}_j(\mathbf{X}^i)$情况下相机的成像函数$\mathcal{P}_j$，通过最小化观测与建模特征点投影之间的距离之和，优化相机的参数。
$$
\sum_{j=1}^{N} \sum_{i=1}^{M} d\left(\mathbf{m}_{j}^{i}, \hat{\mathbf{m}}_{j}^{i}\right)^{2}
$$
​		优化采用的是列文伯格-马夸尔特(LM)算法。

## 2.3 双目鱼眼相机标定

​		

# 3 基于双目鱼眼相机的视觉SLAM技术

​		本文采用的视觉SLAM算法是ORB-SLAM3。ORB-SLAM3是由Carlos Campos与Richard Elvira于2020年论文*提出的。ORB-SLAM3由三个线程组合而成，分别是：跟踪(Tracking)、构建地图(Local Mapping)、回环检测与地图融合(Loop&Map Merging)。结构如图3.1所示：

![image-20210411173519494](C:\Users\LiuZW\AppData\Roaming\Typora\typora-user-images\image-20210411173519494.png)

​		ORB-SLAM3的前端由跟踪和构建地图构成，负责根据多帧连续的图像提供的信息估计相机位姿与特征点位置，给后端提供较好的初始值。后端由回环检测与地图融合构成，负责进行位置感知与回环检测，在大尺度的轨迹与地图上进行子地图拼接与全局优化。

## 3.1 跟踪(Tracking)

​		跟踪线程的主要流程如图3.2所示：



### 3.1.1 提取ORB算子

​		算法对每张图像在八个尺度等级下提取FAST算子，为了保证提取出的FAST算子呈均匀分布，算法对图像网格划分尺度等级，并保证在每个像元提取至少五个角点。如果提取的算子不够或者某些像元没有角点，则会动态调整阈值，反之则会剔除一定的角点，提取完成后算法会对保留的FAST角点计算方向和ORB描述子。

### 3.1.2 初始位姿估计

​		如果上一帧的跟踪是成功的，算法会通过匀速运动模型预测相机位姿，并依据上一帧图像提取的地图点进行搜索。如果与上一帧提取的地图点匹配较少，算法则在上一帧提取的地图点周围进行更广范围地搜索，然后利用搜索到的联系进行位姿优化。

​		如果跟踪丢失，算法会将该帧转换成词袋中的单词，词袋会选取若干关键帧作为备选，以进行全局重定位。算法通过计算ORB算子来获取每个备选关键帧与地图点之间的联系，然后对备选关键帧使用RANSAC(随机采样一致性)迭代，尝试通过PnP算法找到相机位姿，如果找到有足够内联的相机位姿，算法将会对该位姿进行优化并依据该关键帧与地图点之间的联系进行更广范围的搜索。最后，相机位姿会被再优化，如果有足够内联，跟踪将会继续进行。

### 3.1.3 跟踪局部地图

​		一旦算法估计了相机位姿和一组初始的特征匹配，就会将地图投影到当前帧以搜索更多的地图点联系。为了应对大型地图的复杂性，算法仅投影局部地图。该局部地图包含了与当前帧共享地图点的关键帧集合$\mathcal{K}_1$和与$\mathcal{K}_1$相邻的共视图集合$\mathcal{K}_2$，局部地图还有参考关键帧$K_{ref}\in\mathcal{K}_1$，该参考关键帧与当前帧有最多的共享地图点，在当前帧中对在$\mathcal{K}_1$和$\mathcal{K}_2$中有联系的地图点进行如下搜索：

## 3.2 后端

# 4 双目SLAM飞行器的设计与实现

# 5 双目SLAM性能分析

# 6 总结与展望

# 7 参考文献

# 8 致谢


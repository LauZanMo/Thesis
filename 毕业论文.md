# 1 绪论

## 1.1 研究背景

​		四旋翼飞行器，是一种可以垂直起降，自由悬停的飞行器，因其拥有四个呈十字形交叉的螺旋桨的紧凑结构而得名，其相对于固定翼飞机有着起飞场地更加自由且能够悬停的优势，因此，四旋翼飞行器能够在军用或民用领域完成复杂的室内场景任务。

​		如今的飞行器，应用领域相当广泛，主要应用领域如下：

1. 商业：飞行器应用最广的领域是商业领域，常被用于商业宣传、新闻采集、航拍摄影、物流运输、飞行器表演等多种用途。
2. 农业：飞行器在农业领域的应用也越来越多，小型农业飞行器用于进行农情检测与环境气候探测，大型农业飞行器则用于进行辅助授粉与农药喷洒。
3. 能源业：飞行器在能源业通常被用于进行电力巡检、石油管道巡检、天然气管道巡检以及电梯井测量。
4. 救援：飞行器在救援领域一直倍受重视，通常用于进行防汛抗旱，灾害救援，运送医疗物品以及紧急远程诊断。

​		室外飞行器定位主要依赖GPS，GPS的搜星数量是飞行器定位的重要指标，但是，针对无GPS的室内环境下，飞行器则需要使用其他传感器进行定位与导航，同时，室内飞行器的体积与功耗也对算法的复杂程度有所限制，因此，如何在有限的体积与功耗的情况下，使用合适的传感器，提升飞行器的室内定位精度与鲁棒性，是飞行器室内定位的关键。

## 1.2 研究现状

​		国内外关于飞行器室内定位导航方案较多，以下列出几种应用于室内的解决方案：

​		超宽带定位导航。又称为UWB，其使用纳秒级非正弦波窄脉冲传输数据，有很高的时间分辨率，因此被运用到室内定位中，目前大部分室内定位产品运用的是Decawave推出的DW1000芯片，有集成度高，价格低廉的优点，但是，UWB也有信号容易被阻隔，需要提前布置的缺点，在地质勘探等高危场景下并不适用。

​		雷达定位导航。目前主流雷达定位方法主要是激光雷达和毫米波雷达，雷达测量精度高，在较暗环境下相较于视觉表现更好，在自动驾驶领域采用雷达方案的较多。但是，用于三维定位的雷达价格极高，且扫描频率低，不适合高速运动情况下的飞行器进行定位与导航，对于体积和拉力更小的室内无人机，如何承载三维雷达的重量也是室内无人机导航定位的一大难题。

​		视觉定位导航。目前主流视觉SLAM前端方法有特征点法，光流法，直接法，后端方法有滤波器法和图优化法，相较于其他两种方法，特征点法有精度高的优点，但是需要对整张图像进行特征点提取，对算力要求更大，其他两种方法精度稍低，但是胜在速度更快，更适合算力较低的计算平台。图优化法相较于滤波器法精度更高，但是对于地图较大的场景维护速度会下降，滤波器法因为只需要考虑最近的数据，所以速度更快，但是会存在累积误差。

​		视觉定位导航相对于其他定位方法的最主要的优点是传感器价格低廉、重量轻便和获取的信息丰富，完美满足了室内飞行器对于定位传感器的要求。因此，视觉定位也是当前的研究热门之一，以下列出主流的视觉定位算法介绍：

​		ORB-SLAM。ORB-SLAM前端采用了特征点法，提取ORB算子并进行图像之间的匹配，对匹配结果进行随机采样一致性(RANSAC)算法之后构建图优化的框架，对相机位姿与地图点进行光束法平差(BA)，构建局部地图，后端则采用了DBoW2词袋进行回环检测，检测到回环后进行全局光束法平差(BA)，优化全局地图。最近开源的ORB-SLAM3则是引入了Kannala-Brandt的相机模型用于适配鱼眼相机，加入惯导进行视觉-惯导紧组合提升定位精度，以及提出Atlas进行地图管理，在跟踪丢失后尝试建立新地图，并在检测到地图内联时进行多地图融合，增强了鲁棒性。

​		VINS。VINS前端采用了KLT光流法，通过计算图像部分像素运动来跟踪特征点位置，IMU则进行预积分处理，未初始化前，进行视觉惯导对齐与视觉sfm，初始化后，将IMU对相机位姿的预测和对特征点的三角化测量放入滑动窗口中并进行滑动窗口图优化，后端则是将基于视觉构造的残差项和基于IMU构造的残差项放在一起构造成一个联合优化的问题，通过非线性优化进行地图点与位姿进行全局优化。

​		DSO。

​		本文旨在运用机载相机，对机载相机参数、硬件及定位导航策略进行优化，提升飞行器在无GPS情况下室内定位的精度与鲁棒性，从而运用到地质勘探，廊道巡逻等无GPS的复杂环境下。

## 1.3 主要内容与章节安排

​		本文主要的研究内容是设计一套携带鱼眼相机的飞行器SLAM系统，其中包括飞行器结构设计，鱼眼相机T265标定，针对T265在基于特征点的视觉SLAM进行参数优化和飞行器SLAM系统的整合。飞行器结构设计包括飞行器载荷计算与动力选用、飞行器机架构思与搭建；鱼眼相机T265标定包括鱼眼相机投影模型与畸变模型的选用、鱼眼相机的标定；针对T265在基于特征点的视觉SLAM进行参数优化包括调整ORB提取特征点的数量、尺度因子、图像金字塔的数量以及提取Fast角点的阈值；飞行器SLAM系统的整合包括飞行器参数的调优、mavlink通信协议的构建、ROS网络的构建。

​		本文第一章介绍了室内飞行器定位的研究背景和研究现状，第二章分析了鱼眼相机误差模型和标定方法，第三章讲述了基于特征点的ORB-SLAM3视觉SLAM算法，第四章介绍了携带鱼眼相机的飞行器SLAM系统的设计与实现，第五章讲述了对设计的飞行器SLAM系统的性能进行分析，第六章则是对设计的飞行器SLAM系统进行总结，并提出对该系统的改进思路以及对未来的展望。

# 2 双目鱼眼相机误差分析及标定方法

​		鱼眼镜头，是广角镜头中的一种，其焦距小于16mm而且视场角接近或等于180°。在鱼眼镜头拥有着大视场角的优势的同时，其也有着因为光学原理所产生形变的缺点。因此，在近年火热的即时建图与定位(SLAM)研究中，运用最多的是基于针孔相机原理的标准镜头，鱼眼镜头运用并不广泛。

​		基于四旋翼飞行器的视觉SLAM难点有：飞行器相较于小车、手持等场景抖动更大，实时性要求更高，因此，鲁棒的相机位姿估计以及高实时性，是基于飞行器的视觉SLAM必须具备的特点。使用多个鱼眼相机进行刚性耦合，不仅能增加视场角，获取更加丰富的环境特征信息，还能通过相机系统的转换矩阵这一冗余信息恢复地图的尺度，增加位姿估计的鲁棒性以及特征点的持续追踪概率，进而达到飞行器视觉高精度SLAM的目标。

​		由于透视投影模型不适合鱼眼镜头，所以我们采用了一种更加灵活的径向对称的投影模型。

## 2.1 鱼眼相机模型

​		针孔相机的透视投影模型可由以下公式描述：
$$
r=f\tan\theta \tag{2.1 i}
$$
​		其中$\theta$是主光轴与入射光线的夹角，$r$是像面与主点的距离，$f$是镜头的焦距。鱼眼镜头则是经常被设计成符合以下的透视模型之一：
$$
r=2f\tan(\theta/2)\tag{2.2 ii}
$$

$$
r=f\theta\tag{2.3 iii}
$$

$$
r=2f\sin(\theta/2)\tag{2.4 iv}
$$

$$
r=f\sin(\theta)\tag{2.5 v}
$$

​		上述四个模型中最常见的模型为等距投影$(2.3)$，不同的投影模型的曲线如图2.1所示，针孔相机模型与鱼眼相机模型在几何上的差异如图2.2所示。

![image-20210411155256476](C:\Users\LiuZW\AppData\Roaming\Typora\typora-user-images\image-20210411155256476.png)

​		然而，真实的镜头并不完全遵从该建模，为了对所有的鱼眼相机都具有普适性，投影的一般式被设计如下：
$$
r(\theta)=k_1\theta+k_2\theta^3+k_3\theta^5+k_4\theta^7+k_5\theta^9+\cdots\tag{2.6}
$$
​		通常只需要保留前五个系数即可在有限的计算下，式$(2.6)$可以对任何不同的投影曲线有着良好的估计，若$f$为入射光线与归一化的图像坐标之间的映射关系，则有：
$$
\left(\begin{matrix}
x\\
y
\end{matrix}\right)
=r(\theta)
\left(\begin{matrix}
\cos\varphi\\
\sin\varphi
\end{matrix}\right)
=f(\phi)\tag{2.8}
$$

​		考虑到真实镜头并不会理想地对称，加入轴向与径向畸变：
$$
\Delta_r(\theta,\varphi)=
(l_1\theta+l_2\theta^3+l_3\theta^5)
(i_1\cos\varphi+i_2\sin\varphi+i_3\cos2\varphi+i_4sin2\varphi)\tag{2.9}
$$

$$
\Delta_{\mathrm{t}}(\theta, \varphi)=\left(m_{1} \theta+m_{2} \theta^{3}+m_{3} \theta^{5}\right)\left(j_{1} \cos \varphi+j_{2} \sin \varphi+j_{3} \cos 2 \varphi+j_{4} \sin 2 \varphi\right)\tag{2.10}
$$

​		代入$(2.8)$可得
$$
\mathbf{x}_{\mathrm{d}}=r(\theta) \mathbf{u}_{r}(\varphi)+\Delta_{r}(\theta, \varphi) \mathbf{u}_{r}(\varphi)+\Delta_{t}(\theta, \varphi) \mathbf{u}_{\varphi}(\varphi)\tag{2.11}
$$
​		其中$\mathbf{u}_{r}(\varphi)$和$\mathbf{u}_{\varphi}$分别是轴向与径向的单位向量。将相机平面的坐标转换到像平面，则有：
$$
\left(\begin{array}{l}
u \\
v
\end{array}\right)=\left[\begin{array}{cc}
m_{u} & 0 \\
0 & m_{v}
\end{array}\right]\left(\begin{array}{l}
x_{\mathrm{d}} \\
y_{\mathrm{d}}
\end{array}\right)+\left(\begin{array}{l}
u_{0} \\
v_{0}
\end{array}\right)=\mathcal{A}\left(\mathrm{x}_{\mathrm{d}}\right)\tag{2.12}
$$
​		其中$(u_0,v_0)^T$是主点，$m_u$和$m_v$单位是像平面水平方向和垂直方向的单位像素。由$(2.11)$$(2.12)$联立可得总的鱼眼相机模型：
$$
\mathbf{m}=\mathcal{P}_{\mathrm{c}}(\Phi)\tag{2.13}
$$

​		其中$\mathbf{m}=(u,v)^T$，因此，鱼眼相机总共有$23$个参数需要估计，以$\mathbf{p_{23}}$表示。

## 2.2 鱼眼相机标定

​		单目鱼眼相机标定过程总共包含四步。假设相机从$N$个视角观测到$M$个特征点，对于每个视角，若有旋转矩阵$R_j$和位移向量$t_j$描述从相机相对于标定板的位置，则有：
$$
\mathbf{X}_{\mathbf{c}}=\mathbf{R}_{j} \mathbf{X}+\mathbf{t}_{j}, \quad j=1, \ldots, N\tag{2.14}
$$
​		设标定板与$XY$平面重合，则特征点的坐标可表示为$\mathbf{X^i}=(X^i,Y^i,0)$，相应的齐次坐标可表示为$\mathbf{x_p^j}=(X^i,Y^i,1)^T$，在第$j$个视角中的观测坐标为$\mathbf{m_j^i}=(u_j^i,v_j^i)^T$。标定过程的前三步只与六个相机内参有关，缩写为$\mathbf{p}_{6} \hat{=}\left(k_{1}, k_{2}, m_{u}, m_{v}, u_{0}, v_{0}\right)$，其他的参数仅与最后一步有关。

### 2.2.1 内参初始化

​		鱼眼模型$r=k_1\theta+k_2\theta^3$中$k_1$和$k_2$的初始值，是由生产厂商提供的公称焦距$f$和视场角$\theta_{max}$计算所得，并可通过$r_{max}=k_1\theta_{max}+k_2\theta_{max}^3$获取相机平面的图像半径。

​		对于鱼眼镜头，真实图像只在图像帧的一个类圆区域之中，在像平面中，真实图像则在一个椭圆区域之中，有：
$$
\left(\frac{u-u_{0}}{a}\right)^{2}+\left(\frac{v-v_{0}}{b}\right)^{2}=1\tag{2.15}
$$
​		同时还有$m_u=a/r_{max}$和$m_v=b/r_{max}$，因此，可以通过上述计算$\mathbf{p_6}$中的$m_u$，$m_v$，$u_0$和$v_0$。

### 2.2.2 计算逆投影和单应矩阵

​		结合内参$\mathbf{p_6}$，将观测点$\mathbf{m_j^i}$逆投影到以相机为原点的单位球面上，这些点以$\tilde{\mathbf{x}}_{j}^{i}$表示，因为观测点从标定板到单位球面的变换属于透视投影，所以存在单应矩阵$\mathbf{H_j}$有$s\tilde{\mathbf{x}}_{j}^{i}=\mathbf{H_j}\mathbf{x_p^i}$。

​		以下是对于任意视角$j$的单应矩阵$\mathbf{H_j}$的计算方法：

1. 通过计算归一化图像坐标来逆投影特征点：

$$
\left(\begin{array}{l}
x_{j}^{i} \\
y_{j}^{i}
\end{array}\right)=\left[\begin{array}{cc}
1 / m_{u} & 0 \\
0 & 1 / m_{v}
\end{array}\right]\left(\begin{array}{c}
u_{j}^{i}-u_{0} \\
v_{j}^{i}-v_{0}
\end{array}\right)\tag{2.16}
$$

然后将之转化为极坐标$\left(r_{j}^{i}, \varphi_{j}^{i}\right)=\left(x_{j}^{i}, y_{j}^{i}\right)$，最后，通过求解一元三次方程$k_{2}\left(\theta_{j}^{i}\right)^{3}+k_{1} \theta_{j}^{i}-r_{j}^{i}=0$得到值$\theta_{j}^{i}$。

2. 令$\tilde{\mathbf{x}}_{j}^{i}=\left(\sin \varphi_{j}^{i} \sin \theta_{j}^{i}, \cos \varphi_{j}^{i} \sin \theta_{j}^{i}, \cos \theta_{j}^{i}\right)$。
3. 通过线性归一化算法，由$\tilde{\mathbf{x}}_{j}^{i} \leftrightarrow \mathbf{x}_{\mathrm{p}}^{i}$之间的映射关系计算单应矩阵$\mathbf{H_j}$的初始估计值。
4. 通过最小二乘减小$\sum_{i} \sin ^{2} \alpha_{j}^{i}$来优化单应矩阵$\mathbf{H_j}$，其中$\alpha_{j}^{i}$是单位向量$\tilde{\mathbf{x}}_{j}^{i}$与$\hat{\mathbf{x}}_{j}^{i}$的夹角。

### 2.2.3 外参初始化

​		相机外参的初始值是从单应矩阵分解而来的，有：
$$
s \tilde{\mathbf{x}}_{j}^{i}=\left[\begin{array}{ll}
\mathbf{R}_{j} & \mathbf{t}_{j}
\end{array}\right]\left(\begin{array}{c}
X^{i} \\
Y^{i} \\
0 \\
1
\end{array}\right)=\left[\begin{array}{lll}
\mathbf{r}_{j}^{1} & \mathbf{r}_{j}^{2} & \mathbf{t}_{j}
\end{array}\right]\left(\begin{array}{c}
X^{i} \\
Y^{i} \\
1
\end{array}\right)\tag{2.17}
$$
​		其中$\mathbf{H}_{j}=\left[\mathbf{r}_{j}^{1} \mathbf{r}_{j}^{2} \mathbf{t}_{j}\right]$，另有：
$$
\mathbf{r}_{j}^{1}=\lambda_{j} \mathbf{h}_{j}^{1}, \quad \mathbf{r}_{j}^{2}=\lambda_{j} \mathbf{h}_{j}^{2}, \quad \mathbf{r}_{j}^{3}=\mathbf{r}_{j}^{1} \times \mathbf{r}_{j}^{2}, \quad \mathbf{t}_{j}=\lambda_{j} \mathbf{h}_{j}^{3}\tag{2.18}
$$
​		其中$\lambda_{j}=\operatorname{sign}\left(H_{j}^{3,3}\right) /\left\|\mathbf{h}_{j}^{1}\right\|$，由于存在估计误差，计算得到的旋转矩阵不是正交矩阵，所以需要使用奇异值分解的方法计算最接近的正交矩阵，并令其作为$\mathbf{R}_{j}$的初始估计值。

### 2.2.4 投影误差最小化

​		鱼眼模型参数$\mathbf{p_{23}}$的除$\mathbf{p_6}$之外的其他参数初始值设置为$0$，使用式$(2.11)$、$(2.12)$和$(2.14)$来计算特征点满足$\hat{\mathbf{m}}_j^i=\mathcal{P}_j(\mathbf{X}^i)$情况下相机的成像函数$\mathcal{P}_j$，通过最小化观测与建模特征点投影之间的距离之和，优化相机的参数。
$$
\sum_{j=1}^{N} \sum_{i=1}^{M} d\left(\mathbf{m}_{j}^{i}, \hat{\mathbf{m}}_{j}^{i}\right)^{2}
$$
​		优化采用的是列文伯格-马夸尔特(LM)算法。

# 3 基于双目鱼眼相机的视觉SLAM技术

​		本文采用的视觉SLAM算法是ORB-SLAM3。ORB-SLAM3是由Carlos Campos与Richard Elvira于2020年论文*提出的。ORB-SLAM3由三个线程组合而成，分别是：跟踪(Tracking)、构建地图(Local Mapping)、回环检测与地图融合(Loop&Map Merging)。结构如图3.1所示：

![image-20210411173519494](C:\Users\LiuZW\AppData\Roaming\Typora\typora-user-images\image-20210411173519494.png)

​		ORB-SLAM3的前端由跟踪和构建地图构成，负责根据多帧连续的图像提供的信息估计相机位姿与特征点位置，给后端提供较好的初始值。后端由回环检测与地图融合构成，负责进行位置感知与回环检测，在大尺度的轨迹与地图上进行子地图拼接与全局优化。

## 3.1 跟踪(Tracking)

​		对于每一张输入的图像，算法会进行跟踪，跟踪线程的主要流程如图3.2所示：



### 3.1.1 提取ORB算子

​		算法对每张图像在八个尺度等级下提取FAST算子，为了保证提取出的FAST算子呈均匀分布，算法对图像划分尺度等级，并保证在每个像元提取至少五个角点。如果提取的算子不够或者某些像元没有角点，则会动态调整阈值，反之则会剔除一定的角点，提取完成后算法会对保留的FAST角点计算方向和ORB描述子。

### 3.1.2 初始位姿估计

​		如果上一帧的跟踪是成功的，算法会通过匀速运动模型预测相机位姿，并依据上一帧图像提取的地图点进行搜索。如果与上一帧提取的地图点匹配较少，算法则在上一帧提取的地图点周围进行更广范围地搜索，然后利用搜索到的联系进行位姿优化。

​		如果跟踪丢失，算法会将该帧转换成词袋中的单词，词袋会选取若干关键帧作为备选，以进行全局重定位。算法通过计算ORB算子来获取每个备选关键帧与地图点之间的联系，然后对备选关键帧使用RANSAC(随机采样一致性)迭代，尝试通过PnP算法找到相机位姿，如果找到有足够多内联的相机位姿，算法将会对该位姿进行优化并依据该关键帧与地图点之间的联系进行更广范围的搜索。最后，相机位姿会被再优化，如果有足够内联，跟踪将会继续进行。

### 3.1.3 跟踪局部地图

​		一旦算法估计了相机位姿和一组初始的特征匹配，就会将地图投影到当前帧以搜索更多的地图点联系。为了应对大型地图的复杂性，算法仅投影局部地图。该局部地图包含了与当前帧共享地图点的关键帧集合$\mathcal{K}_1$和与$\mathcal{K}_1$相邻的共视图集合$\mathcal{K}_2$，局部地图还有参考关键帧$K_{ref}\in\mathcal{K}_1$，该参考关键帧与当前帧有最多的共享地图点，在当前帧中对在$\mathcal{K}_1$和$\mathcal{K}_2$中有联系的地图点进行如下搜索：

1. 计算地图点在当前帧的投影$\mathbf{x}$，若地图点不在图像内则丢弃。
2. 计算当前视角方向$\mathbf{v}$与地图点平均观测方向之间的夹角$\mathbf{n}$，若$\mathbf{v}\cdot\mathbf{n}<\cos(60°)$则丢弃。
3. 计算地图点到相机中心的距离$d$，若不在地图点的尺度不变区域$(d\notin[d_{min},d_{max}])$则丢弃。
4. 通过计算$d/d_{min}$比例得到当前帧的尺度。
5. 以预测的帧的尺度，对比地图点的代表描述符$\mathbf{D}$与当前帧中仍未匹配的ORB特征，并对帧与地图点计算最优匹配。

在当前帧中所有特征点都被地图点匹配上之后，相机位姿将被优化出来。

### 3.1.4 新关键帧的决策

​		决定是否将当前帧作为关键帧是跟踪的最后一步。因为局部建图有剔除冗余关键帧的机制，且为了让相机在有挑战性的场景下的跟踪更加鲁棒，算法需要尽可能快地插入关键帧，插入关键帧必须满足一下条件：

1. 上一次全局重定位后普通帧必须超过20帧。
2. 局部建图空闲，或自上次关键帧插入以来已经有超过20帧普通帧。
3. 当前帧必须跟踪了至少50个地图点。
4. 当前帧跟踪必须跟踪多于参考帧$K_{ref}$的$90\%$特征点。

## 3.2 局部建图

​		对于每一个新关键帧$K_i$，算法会进行局部建图，局部建图的流程如图3.3所示：



### 3.2.1 关键帧嵌入

​		局部建图首先需要更新共视图，为当前关键帧设置新的顶点$K_i$并根据其他关键帧共享的地图点设置新的边，然后，算法会将有大量共同点的关键帧与当前关键帧$K_i$链接，生成树。其次，算法会计算关键帧的词袋描述，以助于通过三角化测量新特征点来计算数据之间的联系。

### 3.2.2 局部地图点的筛选

​		地图点必须在创建后的前三个关键帧内通过约束测试，再能保留在地图中，这种做法可以保证地图点不会因为虚假的数据关联而被错误地三角化测量。约束如下：

1. 该地图点必须在至少$25\%$的普通帧中可视。
2. 如果该地图点被超过一帧关键帧观察到，则至少要有三个关键帧能观察到该点。

一旦有一个地图点通过测试，其只能在观测到它的关键帧少于三帧时才能被移除。移除操作通常会在关键帧被剔除或者局部BA丢弃离群观测时进行。

### 3.2.3 新地图点的创建

​		通过对共视图中连接的关键帧$\mathcal{K}_c$的ORB算子进行三角化测量，算法可以创建新的地图点，对于关键帧$K_i$未匹配的ORB算子算法会对其他关键帧中的未匹配算子进行匹配搜索，若搜索的匹配不满足极线约束，则丢弃，ORB点对进行三角化测量时，需要检查两个相机的正深度，视差，重投影误差和尺度一致性。刚开始时，地图点仅被两个关键帧观测到，但是它仍然可以被其他关键帧匹配到，因此其能被投影到其他连接的关键帧中。

### 3.2.4 局部光束法平差(BA)

​		局部BA会对当前处理的关键帧$K_i$，其通过共视图连接的所有关键帧$\mathcal{K}_c$，和上述所有关键帧观测到的地图点进行优化。所有的观测到上述地图点但是未通过共视图与当前关键帧$K_i$进行连接的关键帧会被优化包含但是保持不变。被标记为异常值的观测将会在优化的中间和结尾被丢弃。

### 3.2.5 局部关键帧筛选

​		为了维持紧凑的重建，局部建图算法会对冗余关键帧进行检测与删除。因为BA复杂度会随着关键帧的数量的增加而上升，而且为了能保证算法在较小环境下长时间保持稳定性，所以上述操作是有必要的。对于关键帧集$\mathcal{K}_c$，若其$90\%$的地图点能被至少三帧同样或更好的尺度的关键帧观测到，则其将被算法丢弃。利用尺度因子能够保证地图点是通过最高精度关键帧测量得到的。

## 3.3 回环检测和地图融合

​		回环检测获取到局部建图处理的关键帧$K_i$，尝试检测回环，回环检测与地图融合的流程如图3.4所示：

### 3.3.1 回环候选检测

​		首先，算法会计算关键帧$K_i$和其共视图的友邻$(\theta_{min}=30)$的词袋向量的相似度，并保留最低值$s_{min}$，然后，算法会查询位置感知数据库并丢弃所有计算值低于$s_{min}$的关键帧。为了鲁棒性，算法只会在检测到三个一致的回环候选才会接收该回环候选，如果有多个地方的场景与$K_i$相似，则会有多个循环候选。

### 3.3.2 计算相似度

​		算法会计算地图点在当前帧映射的ORB算子与回环候选关键帧的相似度，可以对每个候选使用RANSAC迭代以计算关键帧与候选的相似度，一旦相似度$S_{il}$超过搜索阈值，算法则会依据该候选帧进行更广范围的搜索，以搜寻足够多的内联，相似度$S_{il}$超过成立阈值之后，算法将认为该回环$K_l$成立，对全局关键帧位姿与地图点进行优化。

### 3.3.4 回环融合

​		首先，回环融合会将重复的地图点进行剔除，并将回环的约束作为优化问题的一条边嵌入共视图中，将当前关键帧，也就是触发回环的关键帧，通过相似变换将当前的位姿进行校正，然后再对当前关键帧的邻域进行校正，直至回环首尾对齐，将当前关键帧观测到的地图点进行融合，并且为所有能观测到融合地图点的关键帧进行优化问题边的更新，并将回环这一新的约束作为一条边嵌入这些关键帧的共视图中。

# 4 双目SLAM飞行器的设计与实现

# 5 双目SLAM性能分析

# 6 总结与展望

# 7 参考文献

# 8 致谢

